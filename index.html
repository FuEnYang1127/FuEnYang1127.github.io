<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fu-En Yang</title>
  
  <meta name="author" content="Fu-En Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/nvidia.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fu-En Yang</name>
              </p>
              <!-- <p>I am an incoming Research Scientist at <a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA Research</a> (Feb. 2024). I received Ph.D. from <a href="https://www.ntu.edu.tw/english/index.html" target="_blank">National Taiwan University (NTU)</a> in 2023, under the supervision of Prof. <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a> for the work on "visual understanding with knowledge transfer".
              </p> -->
              <p>I am a Research Scientist at <a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA Research</a>, pursuing research on Adaptive Physical Intelligence, focusing on developing efficient, adaptive AI systems for vision-language-action models (VLA), world modeling, embodied reasoning, and physical AI.
              </p>
              <p>I received my Ph.D. from <a href="https://www.ntu.edu.tw/english/index.html" target="_blank">National Taiwan University (NTU)</a> in Jul. 2023, supervised by Prof. <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>. Previously, I was a research intern at <a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA Research</a> (Feb. 2023-Aug. 2023), focusing on efficient model personalization and vision-language models. Also, I was a Ph.D. program researcher at <a href="https://aics.asus.com/ai-research-en/" target="_blank">ASUS AICS</a> from Sep. 2020 to Oct. 2022, specializing in visual transfer learning.
              </p>
              </p>
              <p>Prior to my Ph.D., I received my Bachelor's degree from <a href="https://web.ee.ntu.edu.tw/eng/index.php" target="_blank">Department of Electrical Engineering</a> at <a href="https://www.ntu.edu.tw/english/index.html" target="_blank">National Taiwan University</a> in 2018. 
              </p>
              <p style="text-align:center">
                <a href="mailto:aoncpc@gmail.com" target="_blank">Email</a> &nbsp/&nbsp
                <a href="data/Fu_En_CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.tw/citations?user=k6Iz9VoAAAAJ&hl=zh-TW" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/fu-en-yang-77ba7b175/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/FuEnYang1" target="_blank">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/FuEnYang1127" target="_blank">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/fuen.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/fuen.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
                <ul>
                  <li> [Jun. 2025] One co-authored paper "LongSplat" is accepted at <a href="https://iccv.thecvf.com/" target="_blank">ICCV 2025</a>.
                    <br>
                  <li> [Feb. 2025] Our paper "VideoMage" is accepted at <a href="https://cvpr.thecvf.com/" target="_blank">CVPR 2025</a>.
                    <br>
                  <li> [Jul. 2024] Our papers "Receler" and "Select and Distill" are accepted at <a href="https://eccv.ecva.net/" target="_blank">ECCV 2024</a>.
                    <br>
                  <li> [Feb. 2024] Join <a href="https://research.nvidia.com/labs/twn/" target="_blank">NVIDIA Research</a> as a Research Scientist.
                    <br>
                  <li> [Jan. 2024] Our paper "RAPPER" is accepted at <a href="https://iclr.cc/" target="_blank">ICLR 2024</a>.
                  <br>
                  <li> [Dec. 2023] Our paper "FedLGT" is accepted at <a href="https://aaai.org/aaai-conference/" target="_blank">AAAI 2024</a>.
                  <br>  
                  <li> [Nov. 2023] Honored to receive the <a href="https://www.taai.org.tw/award/winner" target="_blank">Honorable Mention at 2023 Taiwanese Association for Artificial Intelligence (TAAI) Ph.D. Thesis Award</a>.
                  <br>
                  <li> [Sep. 2023] Honored to receive the <a href="https://www.aca.ntu.edu.tw/w/acaEN/GAADService_23060909163396613" target="_blank">2023 Presidential Award for Graduate Students, National Taiwan University</a>.
                  <br>
                  <li> [Aug. 2023] Honored to receive the <a href="https://140.125.183.142/wp-content/uploads/2023/08/%E7%AC%AC16%E5%B1%86%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E%E7%8D%B2%E7%8D%8E%E5%90%8D%E5%96%AE.pdf" target="_blank">2023 Chinese Image Processing and Pattern Recognition Society (IPPR) Best Doctoral Thesis Award</a>.
                  <br>
                  <li> [Jul. 2023] I officially obtained my Ph.D. degree from <a href="https://www.ntu.edu.tw/english/index.html" target="_blank">National Taiwan University (NTU)</a>.
                  <br>
                  <li> [Jul. 2023] Our paper "<a href="https://arxiv.org/abs/2308.15367" target="_blank">pFedPG</a>" is accepted at <a href="https://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a>.
                </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>
                My research goal is to advance Adaptive Physical Intelligence Research, developing <strong>efficient, adaptive, and customized</strong> AI systems that seamlessly integrate perception, reasoning, and action in physical environments. I focus on vision-language-action models that enable intelligent agents to understand and interact with the world through multimodal reasoning, sophisticated world modeling for predictive understanding of dynamic environments, and embodied reasoning that bridges abstract cognition with physical reality. My work involves developing novel latent modeling approaches to capture the underlying structure of complex physical interactions, with applications spanning target robotics and physical AI systems. I am driven by the vision that AI should not merely process information, but should adaptively learn from and intelligently respond to the rich complexity of physical experience, ultimately creating more capable, personalized, and contextually aware artificial agents. Full list of publications <a href="https://scholar.google.com.tw/citations?user=k6Iz9VoAAAAJ&hl=zh-TW" target="_blank">here</a>.
                <!-- I'm interested in computer vision, deep learning, representation learning, domain adaptation/generalization, few-shot learning, zero-shot learning, and self-supervised learning, and multimodal learning for vision tasks.  -->
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="thinkact_stop()" onmouseover="thinkact_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='thinkact_image'><img src='images/thinkact.png' width="125%"></div>
                <img src='images/thinkact.png' width="125%">
                </div>
                <script type="text/javascript">
                function thinkact_start() {
                    document.getElementById('thinkact_image').style.opacity = "1";
                }

                function thinkact_stop() {
                    document.getElementById('thinkact_image').style.opacity = "0";
                }
                thinkact_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://jasper0314-huang.github.io/thinkact-vla/" target="_blank">
                <papertitle>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</papertitle>
              </a>
              <br>
              <a href="https://jasper0314-huang.github.io/" target="_blank">Chi-Pin Huang</a>,
              <a href="https://kristery.github.io/" target="_blank">Yueh-Hua Wu</a>,
              <a href="https://minhungchen.netlify.app/" target="_blank">Min-Hung Chen</a>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>,
              <strong>Fu-En Yang</strong>
              <br>
							<em>Preprint</em>, 2025 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2507.16815" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2507.16815" target="_blank">arXiv</a>
        /     
              <a href="https://jasper0314-huang.github.io/thinkact-vla/" target="_blank">project</a>
        <!-- / -->
              <!-- <a href="data/fl_poster.pdf" target="_blank">poster</a> -->
        <!-- / -->
              <!-- <a href="https://github.com/WesleyHsieh0806/SS-PRL" target="_blank">code</a> -->
              <p></p>
              We introduce ThinkAct, a reasoning VLA framework capable of thinking before acting. Through reasoning reinforced by our action-aligned visual feedback, ThinkAct enables capabilities of few-shot adaptation, long-horizon planning, and self-correction in embodied tasks.
            </td>
          </tr> 
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="longsplat_stop()" onmouseover="longsplat_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='longsplat_image'><img src='images/longsplat.png' width="125%"></div>
                <img src='images/longsplat.png' width="125%">
                </div>
                <script type="text/javascript">
                function longsplat_start() {
                    document.getElementById('longsplat_image').style.opacity = "1";
                }

                function longsplat_stop() {
                    document.getElementById('longsplat_image').style.opacity = "0";
                }
                longsplat_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://linjohnss.github.io/longsplat/" target="_blank">
                <papertitle>LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos</papertitle>
              </a>
              <br>
              <a href="https://linjohnss.github.io/" target="_blank">Chin-Yang Lin</a>,
              <a href="https://sunset1995.github.io/" target="_blank">Cheng Sun</a>,
              <strong>Fu-En Yang</strong>,
              <a href="https://minhungchen.netlify.app/" target="_blank">Min-Hung Chen</a>,
              <a href="https://sites.google.com/site/yylinweb/" target="_blank">Yen-Yu Lin</a>,
              <a href="https://yulunalexliu.github.io/" target="_blank">Yu-Lun Liu</a>
              <br>
							<em>IEEE International Conference on Computer Vision (ICCV)</em>, 2025 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2508.14041" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2508.14041" target="_blank">arXiv</a>
        /     
              <a href="https://linjohnss.github.io/longsplat/" target="_blank">project</a>
        /
              <a href="https://github.com/NVlabs/LongSplat" target="_blank">code</a>
              <p></p>
              LongSplat reconstructs scenes from any casual long video without camera calibration and renders high-quality novel views from any point along your path.
            </td>
          </tr> 
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="videomage_stop()" onmouseover="videomage_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='receler_image'><img src='images/videomage.png' width="125%"></div>
                <img src='images/videomage.png' width="125%">
                </div>
                <script type="text/javascript">
                function videomage_start() {
                    document.getElementById('videomage_image').style.opacity = "1";
                }

                function videomage_stop() {
                    document.getElementById('videomage_image').style.opacity = "0";
                }
                videomage_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_VideoMage_Multi-Subject_and_Motion_Customization_of_Text-to-Video_Diffusion_Models_CVPR_2025_paper.pdf" target="_blank">
                <papertitle>VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models</papertitle>
              </a>
              <br>
              <a href="https://jasper0314-huang.github.io/" target="_blank">Chi-Pin Huang</a>,
              <a href="https://www.csie.ntu.edu.tw/~b09902097/yensiangwu/" target="_blank">Yen-Siang Wu</a>,
              Hung-Kai Chung,
              Kai-Po Chang,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2503.21781" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2503.21781" target="_blank">arXiv</a>
        /     
              <a href="https://jasper0314-huang.github.io/videomage-customization/" target="_blank">project</a>
        <!-- /
              <a href="data/fl_poster.pdf" target="_blank">poster</a>
        / -->
              <!-- <a href="https://github.com/WesleyHsieh0806/SS-PRL" target="_blank">code</a> -->
              <p></p>
              VideoMage enables text-to-video diffusion models to generate coherent videos with multiple customized subjects and their distinct, controllable motion patterns through disentangled appearanceâ€“motion learning and spatial-temporal composition.
            </td>
          </tr> 
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="receler_stop()" onmouseover="receler_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='receler_image'><img src='images/receler.png' width="125%"></div>
                <img src='images/receler.png' width="125%">
                </div>
                <script type="text/javascript">
                function fl_start() {
                    document.getElementById('receler_image').style.opacity = "1";
                }

                function fl_stop() {
                    document.getElementById('receler_image').style.opacity = "0";
                }
                ssl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.17717" target="_blank">
                <papertitle>Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers</papertitle>
              </a>
              <br>
              <a href="https://jasper0314-huang.github.io/" target="_blank">Chi-Pin Huang</a>,
              Kai-Po Chang,
              Chung-Ting Tsai,
              Yung-Hsuan Lai,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>European Conference on Computer Vision (ECCV)</em>, 2024 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2311.17717" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2311.17717" target="_blank">arXiv</a>
        /     
              <a href="https://jasper0314-huang.github.io/receler-concept-erasing/" target="_blank">project</a>
        /
              <a href="https://github.com/jasper0314-huang/Receler" target="_blank">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="select_stop()" onmouseover="select_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='select_image'><img src='images/select.png' width="125%"></div>
                <img src='images/select.png' width="125%">
                </div>
                <script type="text/javascript">
                function fl_start() {
                    document.getElementById('select_image').style.opacity = "1";
                }

                function fl_stop() {
                    document.getElementById('select_image').style.opacity = "0";
                }
                ssl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2403.09296" target="_blank">
                <papertitle>Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models</papertitle>
              </a>
              <br>
              <a href="https://chuyu.org/" target="_blank">Yu-Chu Yu</a>,
              <a href="https://jasper0314-huang.github.io/" target="_blank">Chi-Pin Huang</a>,
              Jr-Jen Chen,
              Kai-Po Chang,
              Yung-Hsuan Lai,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>European Conference on Computer Vision (ECCV)</em>, 2024 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2403.09296" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2403.09296" target="_blank">arXiv</a>
        /     
              <a href="https://chuyu.org/research/snd/" target="_blank">project</a>
        /
              <a href="https://github.com/chu0802/SnD" target="_blank">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>





        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="rapper_stop()" onmouseover="rapper_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rapper_image'><img src='images/rapper.png' width="125%"></div>
                <img src='images/rapper.png' width="125%">
                </div>
                <script type="text/javascript">
                function fl_start() {
                    document.getElementById('rapper_image').style.opacity = "1";
                }

                function fl_stop() {
                    document.getElementById('rapper_image').style.opacity = "0";
                }
                ssl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=bshfchPM9H" target="_blank">
                <papertitle>RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering</papertitle>
              </a>
              <br>
              Kai-Po Chang,
              <a href="https://jasper0314-huang.github.io/" target="_blank">Chi-Pin Huang</a>, 
              Wei-Yuan Cheng,
              <strong>Fu-En Yang</strong>,
              <a href="https://chienyiwang.github.io/" target="_blank">Chien-Yi Wang</a>,
              Yung-Hsuan Lai,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>International Conference on Learning Representations (ICLR)</em>, 2024 &nbsp
              <br>
              <a href="https://openreview.net/pdf?id=bshfchPM9H" target="_blank">paper</a>
        <!-- /
              <a href="https://arxiv.org/abs/2308.15367" target="_blank">arXiv</a> -->
        <!-- /     
              <a href="data/fl_slides.pdf" target="_blank">slides</a> -->
        <!-- /
              <a href="data/fl_poster.pdf" target="_blank">poster</a> -->
        <!-- / -->
              <!-- <a href="https://github.com/WesleyHsieh0806/SS-PRL" target="_blank">code</a> -->
              <p></p>
            </td>
          </tr> 
        </tbody></table>


        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="fedlgt_stop()" onmouseover="fedlgt_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fedlgt_image'><img src='images/fedlgt.png' width="125%"></div>
                <img src='images/fedlgt.png' width="125%">
                </div>
                <script type="text/javascript">
                function fl_start() {
                    document.getElementById('fedlgt_image').style.opacity = "1";
                }

                function fl_stop() {
                    document.getElementById('fedlgt_image').style.opacity = "0";
                }
                ssl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.07165" target="_blank">
                <papertitle>Language-Guided Transformer for Federated Multi-Label Classification</papertitle>
              </a>
              <br>
              <a href="https://jack24658735.github.io/" target="_blank">I-Jieh Liu</a>,
              Ci-Siang Lin,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2312.07165.pdf" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2312.07165" target="_blank">arXiv</a>
        /     
              <a href="https://jack24658735.github.io/fedlgt/" target="_blank">webpage</a>
        /
              <!-- <a href="data/fl_poster.pdf" target="_blank">poster</a>
        / -->
              <a href="https://github.com/Jack24658735/FedLGT" target="_blank">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="fl_stop()" onmouseover="fl_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fl_image'><img src='images/fl.png' width="125%"></div>
                <img src='images/fl.png' width="125%">
                </div>
                <script type="text/javascript">
                function fl_start() {
                    document.getElementById('fl_image').style.opacity = "1";
                }

                function fl_stop() {
                    document.getElementById('fl_image').style.opacity = "0";
                }
                ssl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Efficient_Model_Personalization_in_Federated_Learning_via_Client-Specific_Prompt_Generation_ICCV_2023_paper.html" target="_blank">
                <papertitle>Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation</papertitle>
              </a>
              <br>
              <strong>Fu-En Yang</strong>,
              <a href="https://chienyiwang.github.io/" target="_blank">Chien-Yi Wang</a>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE International Conference on Computer Vision (ICCV)</em>, 2023 &nbsp
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Efficient_Model_Personalization_in_Federated_Learning_via_Client-Specific_Prompt_Generation_ICCV_2023_paper.pdf" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2308.15367" target="_blank">arXiv</a>
        <!-- /     
              <a href="data/fl_slides.pdf" target="_blank">slides</a> -->
        /
              <a href="data/fl_poster.pdf" target="_blank">poster</a>
        <!-- / -->
              <!-- <a href="https://github.com/WesleyHsieh0806/SS-PRL" target="_blank">code</a> -->
              <p></p>
            </td>
          </tr> 
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="zsl_stop()" onmouseover="zsl_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='zsl_image'><img src='images/zsl.png' width="125%"></div>
                <img src='images/zsl.png' width="125%">
                </div>
                <script type="text/javascript">
                function zsl_start() {
                    document.getElementById('zsl_image').style.opacity = "1";
                }

                function zsl_stop() {
                    document.getElementById('zsl_image').style.opacity = "0";
                }
                ssl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://link.springer.com/article/10.1007/s11263-023-01767-0" target="_blank">
                <papertitle>Semantics-Guided Intra-Category Knowledge Transfer for Generalized Zero-Shot Learning</papertitle>
              </a>
              <br>
              <strong>Fu-En Yang</strong>,
              Yuan-Hao Lee,
              Chia-Ching Lin,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>International Journal of Computer Vision (IJCV)</em>, 2023 &nbsp
              <br>
              <!-- <a href="https://arxiv.org/pdf/2208.14439.pdf" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2208.14439" target="_blank">arXiv</a>
        /
              <a href="https://github.com/WesleyHsieh0806/SS-PRL" target="_blank">code</a> -->
              <p></p>
            </td>
          </tr> 
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="ssl_stop()" onmouseover="ssl_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ssl_image'><img src='images/ssl.png' width="125%"></div>
                <img src='images/ssl.png' width="125%">
                </div>
                <script type="text/javascript">
                function ssl_start() {
                    document.getElementById('ssl_image').style.opacity = "1";
                }

                function ssl_stop() {
                    document.getElementById('ssl_image').style.opacity = "0";
                }
                ssl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2208.14439" target="_blank">
                <papertitle>Self-Supervised Pyramid Representation Learning for Multi-Label Visual Analysis and Beyond</papertitle>
              </a>
              <br>
              Cheng-Yen Hsieh,
              Chih-Jung Chang,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 2023 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2208.14439.pdf" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2208.14439" target="_blank">arXiv</a>
        /
              <a href="https://github.com/WesleyHsieh0806/SS-PRL" target="_blank">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="dgneurips21_stop()" onmouseover="dgneurips21_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dgneurips21_image'><img src='images/dgneurips21.png' width="125%"></div>
                <img src='images/dgneurips21.png' width="125%">
                </div>
                <script type="text/javascript">
                function dgneurips21_start() {
                    document.getElementById('dgneurips21_image').style.opacity = "1";
                }

                function dgneurips21_stop() {
                    document.getElementById('dgneurips21_image').style.opacity = "0";
                }
                dgneurips21_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/a2137a2ae8e39b5002a3f8909ecb88fe-Abstract.html" target="_blank">
                <papertitle>Adversarial Teacher-Student Representation Learning for Domain Generalization</papertitle>
              </a>
              <br>
              <strong>Fu-En Yang</strong>,
              Yuan-Chia Cheng,
              Zu-Yun Shiau,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 &nbsp <font color="red"><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a href="https://proceedings.neurips.cc/paper/2021/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf" target="_blank">paper</a>
        /
              <a href="https://openreview.net/forum?id=gKyyBfMM4Y" target="_blank">OpenReview</a>
        /
              <a href="https://slideslive.com/38967080/adversarial-teacherstudent-representation-learning-for-domain-generalization?ref=recommended" target="_blank">video</a>
        /
              <a href="data/dg_slides.pdf" target="_blank">slides</a>
        /
              <a href="data/dg_poster.pdf" target="_blank">poster</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="fslseg_stop()" onmouseover="fslseg_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fslseg_image'><img src='images/fslseg.png' width="125%"></div>
                <img src='images/fslseg.png' width="125%">
                </div>
                <script type="text/javascript">
                function fslseg_start() {
                    document.getElementById('fslseg_image').style.opacity = "1";
                }

                function fslseg_stop() {
                    document.getElementById('fslseg_image').style.opacity = "0";
                }
                fslseg_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/WACV2022/html/Lee_A_Pixel-Level_Meta-Learner_for_Weakly_Supervised_Few-Shot_Semantic_Segmentation_WACV_2022_paper.html" target="_blank">
                <papertitle>A Pixel-Level Meta-Learner for Weakly Supervised Few-Shot Semantic Segmentation</papertitle>
              </a>
              <br>
              Yuan-Hao Lee,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 2022 &nbsp
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Lee_A_Pixel-Level_Meta-Learner_for_Weakly_Supervised_Few-Shot_Semantic_Segmentation_WACV_2022_paper.pdf" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/2111.01418" target="_blank">arXiv</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="layout_stop()" onmouseover="layout_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='layout_image'><img src='images/layout.png' width="125%"></div>
                <img src='images/layout.png' width="125%">
                </div>
                <script type="text/javascript">
                function layout_start() {
                    document.getElementById('layout_image').style.opacity = "1";
                }

                function layout_stop() {
                    document.getElementById('layout_image').style.opacity = "0";
                }
                layout_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Yang_LayoutTransformer_Scene_Layout_Generation_With_Conceptual_and_Spatial_Diversity_CVPR_2021_paper.html" target="_blank">
                <papertitle>LayoutTransformer: Scene Layout Generation With Conceptual and Spatial Diversity</papertitle>
              </a>
              <br>
              <a href="https://joeyy5588.github.io/" target="_blank">Cheng-Fu Yang*</a>,
              <a href="https://sites.google.com/view/wancyuanfan" target="_blank">Wan-Cyuan Fan*</a>,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021 &nbsp
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_LayoutTransformer_Scene_Layout_Generation_With_Conceptual_and_Spatial_Diversity_CVPR_2021_paper.pdf" target="_blank">paper</a>
        /
              <a href="https://github.com/davidhalladay/LayoutTransformer" target="_blank">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="fsl_stop()" onmouseover="fsl_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fsl_image'><img src='images/fsl.png' width="125%"></div>
                <img src='images/fsl.png' width="125%">
                </div>
                <script type="text/javascript">
                function fsl_start() {
                    document.getElementById('fsl_image').style.opacity = "1";
                }

                function fsl_stop() {
                    document.getElementById('fsl_image').style.opacity = "0";
                }
                fsl_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9506141" target="_blank">
                <papertitle>Few-Shot Classification in Unseen Domains by Episodic Meta-Learning Across Visual Domains</papertitle>
              </a>
              <br>
              Yuan-Chia Cheng,
              Ci-Siang Lin,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE International Conference on Image Processing (ICIP)</em>, 2021 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2112.13539.pdf" target="_blank">paper</a>
        /
              <a href="https://ieeexplore.ieee.org/document/9506141" target="_blank">IEEE Xplore</a>
        /
              <a href="https://arxiv.org/abs/2112.13539" target="_blank">arXiv</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="face_stop()" onmouseover="face_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='face_image'><img src='images/face.png' width="125%"></div>
                <img src='images/face.png' width="125%">
                </div>
                <script type="text/javascript">
                function face_start() {
                    document.getElementById('face_image').style.opacity = "1";
                }

                function face_stop() {
                    document.getElementById('face_image').style.opacity = "0";
                }
                face_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.html" target="_blank">
                <papertitle>Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment</papertitle>
              </a>
              <br>
              Po-Hsiang Huang,
              <strong>Fu-En Yang</strong>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020 &nbsp
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.pdf" target="_blank">paper</a>
        /
              <a href="https://www.youtube.com/watch?v=5NYI_ncWvnI" target="_blank">video</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="i2v_stop()" onmouseover="i2v_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='i2v_image'><img src='images/i2v.png' width="125%"></div>
                <img src='images/i2v.png' width="125%">
                </div>
                <script type="text/javascript">
                function i2v_start() {
                    document.getElementById('i2v_image').style.opacity = "1";
                }

                function i2v_stop() {
                    document.getElementById('i2v_image').style.opacity = "0";
                }
                i2v_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9412781" target="_blank">
                <papertitle>Dual-MTGAN: Stochastic and Deterministic Motion Transfer for Image-to-Video Synthesis</papertitle>
              </a>
              <br>
              <strong>Fu-En Yang</strong>*,
              Jing-Cheng Chang*,
              Yuan-Hao Lee,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE International Conference on Pattern Recognition (ICPR)</em>, 2020 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2102.13329.pdf" target="_blank">paper</a>
        /
              <a href="https://ieeexplore.ieee.org/document/9412781" target="_blank">IEEE Xplore</a>
        /
              <a href="https://arxiv.org/abs/2102.13329" target="_blank">arXiv</a>
        /
              <a href="https://drive.google.com/file/d/1vv4035f-9cuSmt2FobYNuDfO92s1ss_M/view?usp=sharing" target="_blank">video</a>
        /
              <a href="data/ICPR_2020_slides.pdf" target="_blank">slides</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="semanticicpr_stop()" onmouseover="semanticicpr_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='semanticicpr_image'><img src='images/semanticicpr.png' width="125%"></div>
                <img src='images/semanticicpr.png' width="125%">
                </div>
                <script type="text/javascript">
                function semanticicpr_start() {
                    document.getElementById('semanticicpr_image').style.opacity = "1";
                }

                function semanticicpr_stop() {
                    document.getElementById('semanticicpr_image').style.opacity = "0";
                }
                semanticicpr_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9413171" target="_blank">
                <papertitle>Semantics-Guided Representation Learning with Applications to Visual Synthesis</papertitle>
              </a>
              <br>
              Jia-Wei Yan,
              Ci-Siang Lin,
              <strong>Fu-En Yang</strong>,
              <a href="https://yujheli.github.io/" target="_blank">Yu-Jhe Li</a>,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE International Conference on Pattern Recognition (ICPR)</em>, 2020 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2010.10772.pdf" target="_blank">paper</a>
        /
              <a href="https://ieeexplore.ieee.org/document/9413171" target="_blank">IEEE Xplore</a>
        /
              <a href="https://arxiv.org/abs/2010.10772" target="_blank">arXiv</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="m2rd_stop()" onmouseover="m2rd_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='m2rd_image'><img src='images/m2rd.png' width="125%"></div>
                <img src='images/m2rd.png' width="125%">
                </div>
                <script type="text/javascript">
                function m2rd_start() {
                    document.getElementById('m2rd_image').style.opacity = "1";
                }

                function semanticicpr_stop() {
                    document.getElementById('m2rd_image').style.opacity = "0";
                }
                m2rd_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8902223" target="_blank">
                <papertitle>A Multi-Domain and Multi-Modal Representation Disentangler for Cross-Domain Image Manipulation and Classification</papertitle>
              </a>
              <br>
              <strong>Fu-En Yang</strong>*,
              Jing-Cheng Chang*,
              Chung-Chi Tsai,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE Transactions on Image Processing (TIP)</em>, 2020 &nbsp
              <br>
              <a href="data/m2rd.pdf" target="_blank">paper</a>
        /
              <a href="https://ieeexplore.ieee.org/document/8902223" target="_blank">IEEE Xplore</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="vdosum_stop()" onmouseover="vdosum_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vdosum_image'><img src='images/vdosum.png' width="125%"></div>
                <img src='images/vdosum.png' width="125%">
                </div>
                <script type="text/javascript">
                function vdosum_start() {
                    document.getElementById('vdosum_image').style.opacity = "1";
                }

                function semanticicpr_stop() {
                    document.getElementById('vdosum_image').style.opacity = "0";
                }
                vdosum_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8803639?denied=" target="_blank">
                <papertitle>Learning Hierarchical Self-Attention for Video Summarization</papertitle>
              </a>
              <br>
              Yen-Ting Liu,
              <a href="https://yujheli.github.io/" target="_blank">Yu-Jhe Li</a>,
              <strong>Fu-En Yang</strong>,
              Shang-Fu Chen,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE International Conference on Image Processing (ICIP)</em>, 2019 &nbsp
              <br>
              <a href="https://ieeexplore.ieee.org/document/8803639?denied=" target="_blank">IEEE Xplore</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr onmouseout="reid_stop()" onmouseover="reid_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='reid_image'><img src='images/reid.png' width="125%"></div>
                <img src='images/reid.png' width="125%">
                </div>
                <script type="text/javascript">
                function reid_start() {
                    document.getElementById('reid_image').style.opacity = "1";
                }

                function reid_stop() {
                    document.getElementById('reid_image').style.opacity = "0";
                }
                reid_stop()
                </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/w6/html/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.html" target="_blank">
                <papertitle>Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</papertitle>
              </a>
              <br>
              <a href="https://yujheli.github.io/" target="_blank">Yu-Jhe Li</a>,
              <strong>Fu-En Yang</strong>,
              <a href="https://ycliu93.github.io/" target="_blank">Yen-Cheng Liu</a>,
              <a href="https://yuyingyeh.github.io/" target="_blank">Yu-Ying Yeh</a>,
              Xiaofei Du,
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
              <br>
							<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, 2018 &nbsp
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w6/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.pdf" target="_blank">paper</a>
        /
              <a href="https://arxiv.org/abs/1804.09347" target="_blank">arXiv</a>
        /
              <a href="https://github.com/yujheli/ARN" target="_blank">code</a>
              <p></p>
            </td>
          </tr> 
        </tbody></table>

					
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Academic Services</heading>
                  <ul>
                    <li>Conference Program Committee/Reviewer: WACV 2026, NeurIPS 2025, ICCV 2025, ICML 2025, CVPR 2025, ICLR 2025, ICLR 2025 WS SCOPE, AAAI 2025, ACM MM 2025, NeurIPS 2024, ECCV 2024, ICML 2024, CVPR 2024, AAAI 2024, ACCV 2024, ICIP 2024, NeurIPS 2023, ICCV 2023, CVPR 2023, AAAI 2023, WACV 2023, ICIP 2023, ACCV 2022, CVPR 2022, AAAI 2022, WACV 2022, AAAI 2021, ICIP 2020, AAAI 2020
                  </ul>
                  <ul>
                    <li>Journal Reviewer: Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Computer Vision and Image Understanding (CVIU), ACM Computing Surveys (CSUR)
                  </ul>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Awards</heading>
                  <ul>
                    <li>Honorable Mention at 2023 TAAI Ph.D. Thesis Award, Nov. 2023
                    <br>
                    <li>NTU Presidential Award for Graduate Students, Sep. 2023
                    <br>
                    <li>Merit Award at the 16th IPPR Doctoral Thesis Award, Aug. 2023
                  </ul>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Teaching Assistant</heading>
                  <ul>
                    <li>Deep Learning for Computer Vision, Spring 2019
                    <br>
                    <li>Computer Vision: from recognition to geometry, Fall 2018
                  </ul>
              </td>
            </tr>
          </tbody></table>

					
					<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">The <a href="https://github.com/jonbarron/website" target="_blank">template</a> is designed and shared by Dr. <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>.</p>
                  <!-- <br> -->
                </p>
                <p style="text-align:center;font-size:small;">
                
                  <a target="_blank" href="https://clustrmaps.com/site/1bqtk"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=KVWP0R4tKoozSeVBoMOM7DdIhnGKJuHk-qMxrsyiQbo&cl=ffffff" /></a>
                  
              </td>
            </tr>
          </tbody></table>

        
</body>

</html>